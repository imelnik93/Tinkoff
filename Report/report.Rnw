\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks, linkcolor=blue}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\linespread{1.3}
\renewcommand{\familydefault}{\sfdefault}
\author{Anastasia Ivanova, Maxim Savenkov}
\title{Estimation of default probability using bank account data (Tinkoff challenge)}
\date{}
\begin{document}
\SweaveOpts{concordance=TRUE, echo=FALSE, cache=TRUE}
<<libs>>=
setwd('~/Dropbox/Work/Freelance/Tinkoff/Report/')
library(knitr)
library(ggplot2)
@


\maketitle
\section{Introduction}

For this task we decided to use data from Tinkoff machine learning challenge 2013. As they suggested, we try to predict the probability of a loan payment being overdue within the first year of loan given information on customers' credit bank accounts. Such tasks appear when banks want to decide whether they should give a loan to a customer, or they should not. We use logistic regression for this purpose, because it is better suited than linear regression and simple enough to implement. 

A more thorough problem statement and details of the original challenge can be found in the next section. A brief overview of data provided is in section 3. Sections 4 and 5 are entirely built on our scripts "data\_filtering.R" and "analysis.R" respectively and are aimed to give interpretation to what are we doing in these scripts. Section 6 concludes.
\section{Problem statement}
Every credit institution faces certain risks when giving loans to people or firms. No bank wants to lose a trustworthy loanee or give loans to untrustworthy ones. In order to minimize losses due to such risks, banks put their clients through careful vetting procedures before approving a loan. However, manual checks may be costly in terms of time and human resources, but a total lack of such checks poses a threat to bank's stability. Thus, creating a reliable algorithm that can reveal the type ("good" or "bad") of a loanee may substantially increase expected gains of a credit institution while reducing risks.

This task was originally included in Tinkoff machine learning tournament in 2013 (available \href{https://www.tinkoff.ru/tournament/}{here}). We aim to predict the probability of default (credit payment being overdue for more than 90 days during the first year, as stated in the task) using data on bank accounts provided by the challenge authors. Ideally, one should also take into account economic information (for example, currency exchange rates when estimating risks of loans taken in foreign currency) and information about individual performance of entities (data on salary and employment history for people and financial statements for firms), but we chose to stick to the provided dataset since it has abundant information about loanee's credit history, but doesn't make analysis overly complex. A more elaborate discussion of restrictions of such approach can be found in the next section.
\section{Data understanding}
Details regarding particular variables (brief description and types) are included in the attached archive. This section justifies our choice of variables in the model. Challenge authors provided 2 datasets: one with 50000 customer IDs and respective "default" dummies (separated into test and train subsets, the former being only meaningful for the challenge) and the other with detailed information regarding credit history of borrowers (a total 280942 entries). The first dataset is used solely as a source for the dependent variable, so we can focus on the second one.

The correspondence between IDs (and the dependent variable) and account data is not 1-to-1, because for every entity there can be several account histories. There are a few ways to handle this: either use some statistic to use as a proxy for all account data of a particular customer (for example, mean for numeric values and mode for categorical ones), assign respective values of dependent variables to every ID's account entry, or analyse data as is extracting information from "sequences" of accounts throughout time. Obviously, the last approach is the most preferable, but inadequately complex. The first one could be justified in simpler cases, but is not easily implemented here. Thus, we decided to stick to the second approach.

Some variables can be excluded right away because they are either within the chosen approach or for our purpose in general. Examples of these variables are: \textit{bureau\_cd} (the bureau that provided account data), \textit{bki\_request\_date} (date when the bureau request was made), \textit{open\_date} (the data of account creation, useless if we don't consider "account sequences"), etc. A summary of variables we use may be found in the next section.

\section{Data preparation}
First of all, the part of the sample reserved for the challenge submission was excluded. Some variables were deleted right away. Except for the variables mentioned above, we also deleted \textit{final\_pmt\_date}, \textit{inf\_confirm\_date}, \textit{fact\_close\_date} (same reason as \textit{open\_date}), \textit{pmt\_string\_84m} (it is included in \textit{ttl\_delq\_xx} dummies), \textit{status} (it is unclear how to include this in the model), \textit{outstanding} and \textit{next\_pmt} (not informative without the full history of payments).

After that, we kept only entries with \textit{"relationship==1"}, because all codes but 1 and 9 are reserved for indirect payees, and firms (code 9) require a separate model, but lack in numbers (only 205 entries including train sample). Thus, this column contains only 1 value and thus was deleted after sampling.

Most of variables didn't have NAs, but some had them in abundance, e.g., \textit{max\_delq\_balance}. This variable has 0's, other numeric values and NAs. Since we cannot interpret NA in this case (those who had no allowed debt have 0), we deleted this variable. Since \textit{current\_delq}, \textit{curr\_balance\_amt} and \textit{delq\_balance} have similar problems, we excluded them as well.

As the final step, we filtered out values of \textit{pmt\_freq} that have either too few occurrences (less than 10 as a rule of thumb) or code 7 (we cannot say anything about them). We also deleted values of \textit{type} with codes 4, 14, and 99 (the latter is not informative, the first 2 have less than 10 occurrences). 

We transformed 3 variables. According to the intuition we used, for people who have to pay every week it is less probable to allow for a debt to exist for 90+ days. Thus, the higher the frequency of payment, the lesser the probability of default. We assigned numeric values to \textit{pmt\_freq} by dividing a calendar year by the base period of payments, which yields the supposed number of payments per year. Surprisingly, there were 21504 observations with code 0, which is not included in variable description. However arguable this step is, we decided to exclude those (along with 145 empty entries). Probably it would have been better to assume monthly payments (mode of this variable), but thus we can distort the results. Some other levels were excluded as too rare (<10 entries).

We also transformed factor variables \textit{currency} and \textit{type}. We assigned labels for types according to its description and expanded these 2 variables as dummies. The final dataframe was merged by tcs\_customer\_id (excluded during merging) from 2 datasets provided. It can be obtained by using the data\_filtering script from data available online.

\section{Model building and evaluation}

For this model we must use models with binary dependent variables. The most obvious choice is to use logistic regression. As the first step, we tried to include all variables from the final dataset. Here is the summary of the output regression:
<<r, cache=TRUE>>=
df <- read.csv2('df.csv')
reg <- glm(formula = bad ~ ., family = binomial(link = "logit"), data = df)
summary(reg)
@
We can see a few interesting results. First of all, there are 2 coefficients that seem to be NA. This happened because we did not choose and exclude the "base" dummies when trying to expand \textit{type} and \textit{currency} factors. Thus, one dummy per group becomes 100\% dependent on other dummies within its group. We might exclude \textit{currencyRUB} dummy from the regression, because the vast majority of loans is taken in this currency in the sample. As for the \textit{type} variable group, it seems irrelevant to our model.

Another interesting fact is that payment frequency positively affects the probability of default. We might assume a non-linear relation between these variables and include variable \textit{pmt\_freq\_sq}, that is pmt\_freq squared. For obvious reasons, \textit{ttl\_delq\_60\_89} is significant. But we can see that variables within this group are strongly correlated if the periods are close. We might use only 60-89 and 30 days since they are weakly correlated ($\rho$=0.15) and can tell us something about the client's discipline.

Coefficients for variables \textit{interest\_rate} and \textit{credit\_limit} are also significant. We assume that \textit{credit\_limit} may be negatively correlated with the probability of default due to the fact that this variable may be a proxy for individual characteristics or other unobservables of the customer (e.g., discipline or employability), since the better his or her reputation is the larger loan will be approved. It is not as simple to find a good interpretation for the \textit{interest\_rate} coefficient. We speculate that this may be due to the fact that the loanee does not want to pay larger interest on missed payments. However we believe that this interpretation is quite weak.

From the results above we decided to run another model. We decided to include \textit{typecard} and \textit{typemort} here just in case. Summary:
<<>>=
df$pmt_freq_sq <- df$pmt_freq^2
reg2 <- glm(formula = bad ~ credit_limit+ttl_delq_30+ttl_delq_60_89+interest_rate+typecard+typemort+pmt_freq+pmt_freq_sq+currencyEUR+currencyUSD, 
            family = binomial(link = "logit"), data = df)
summary(reg2)
@
We can see that this model is better by looking at AIC. The second model has better results, but we can still cut out \textit{currencyEUR} since it is insignificant.
<<>>=
reg3 <- glm(formula = bad ~ credit_limit+ttl_delq_30+ttl_delq_60_89+interest_rate+typecard+typemort+pmt_freq+pmt_freq_sq+currencyUSD, family = binomial(link = "logit"),
            data = df)
summary(reg3)
@
While this model is marginally worse than the second one, all its coefficients are significant, so we will not change it. Positive and negative correlation between the dependent variable and \textit{typemort} and \textit{typecard} respectively may be explained as the effect of payment sizes for each of these types (assuming that larger payment sizes increase the probability of default). The negative effect of \textit{currencyUSD} for a dataset from 2013 is adequate and may be either due to lower rates on such loans (it is not confirmed by correlation) or due to higher income of loanees. As for the effect of \textit{pmt\_freq}, we can see that it is a parabola with minimum at 26 payments per months and maximum (in our sample) in 52 payments (the effect here is actually positive), which means that weekly payments lead to higher risks.
\section{Discussion}
This model can be used in order to determine whether a certain loanee is trustworthy. Some results must be re-evaluated (the effect of currency, e.g.) to fit in today's realia, it is likely that the model can be improved by certain data (loanee's salary).

As it was already mentioned, there might be better approaches to fit these data like considering sequences of account history. Moreover, this model works only for people, but not firms. However, we believe that our approach can be a used to determine a rough estimate of client's credibility. 
\end{document}